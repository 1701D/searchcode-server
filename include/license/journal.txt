Going to keep a record of progress and thoughts while doing this in the hopes that I get some sort of useful blog post out of it.

Once of the tickets raised for searchcode-server is the ability to to filter by licence https://github.com/boyter/searchcode-server/issues/96 yes, I am the one who raised the ticket, but it is based on requests from customers. It will also put searchcode server closer to feature parity with Krugle so it seems like a good thing to add.

My first thought was that adding it for say the top 20 most popular licenses shouldn't be the difficult, and its something that I could always expand on later.

So the first thing I needed was to determine the top software licenses in use which thankfully blackduck has already done https://www.blackducksoftware.com/top-open-source-licenses 

Then need to get a copy of them and in a nice format such as JSON. Decided to source them from SPDX since they should be considered the source of truth for this https://spdx.org/licenses/

Wrote a simple script download.py to pull them all down (yes using regex to pull information out of HTML which is BADtm but im not trying to parse it so I am pretty sure he won't come). Sorry SPDX people about crawling your site in a non nice way... 

Added a simple filter to pull back the top 20+ licences so we can test.

Next needed to turn the HTML into JSON. Again an ugly script which calls the beast by using regex to pull information out of HTML. One thing I did notice is that the Fair Source licence was missing. Since I was planning on using searchcode server as a test bed I needed that and added it myself. 

Once again sorry to the SPDX people. If you come to Sydney and ill buy you a beer and apologize for,

1) For creating my own shortname for the Fair Source License without permission
2) For the crappy internet you will experience (Seriously search for Turnbull NBN if you want to see how backwards a country can become, don't want to get political here, but WHY would you ever stop rolling out FTTP and then switch to FTTN!?!?!)

The result is now we have a database of about 20 licences that we can use to try and determine what licence a software project is.

Attempt 1

Made the following assumptions.

A file with a name such as license or copying is likely to exist in the root folder of any project and contain license information. If not have a look inside readme (if it exists). This is the base license for all files inside the project.
Files may have a header which overrides the above.

One thing that I didn't consider is that there may be another license/copying file somewhere deeper inside the file tree. Something to consider for later.

First thought was to just use the vector space search algorith. Its kinda my hammer for every problem. It works reasonably well for a lot of them, is fast enough in most cases and generally gets the job done. Thankfully I had already written one for python a while ago. Don't hate please, I wrote this 10 years ago when I was first learning it and yes its not Pythonic but works. One thing to note is that licenses tend to be of different length. This means that licenses that are closer to each other in length will be matched more closesly using the vector space. This is a cool result.

So the algorithm is,

Find likely candidates for license files.
Compare them to the list of known licenses using the vector space model.
Keep the most likely match.

Then walk through every file in the repository, checking for the special header and when there are no matches try the full header because things like MIT license tend to get included at the top of the file.

The result for searchcode-server

Project License
0.929696395964 Fair Source License v0.9
0.802095284986 Mozilla Public License 2.0 (no copyleft exception)
0.802095284986 Mozilla Public License 2.0

0.925341443302 MIT License /Users/boyter/Documents/Projects/searchcode-server/src/main/resources/public/js/cache.js

Wow. That is a very cool result. It actually worked. It not only picked up that its using the Fair Source Licence it also picked up that that one file is using the MIT license. Lets try it out on some other projects.

Against wordpress,

Project License
0.623430083878 GNU General Public License v2.0 only
0.614318516008 GNU General Public License v1.0 only
0.601642491832 GNU Library General Public License v2 only

Hmmm it did pick up that its probably using GNU GPL v2.0 but it wasn't as confident as it was with the previous. Lets try another one.

minitwit (spark java)

Project License
0.954897366777 MIT License
0.784597744861 Fair Source License v0.9
0.777231345803 Apache License 2.0

Not bad. Its pretty confident that it us under the MIT license.

armory-react

Project License
0.945769202843 BSD 3-clause Clear License
0.937649791859 BSD with attribution
0.927894236317 BSD 2-clause "Simplified" License

Again pretty good. 

Ok. So it looks like we could just pop the top license off and call it a day. This works pretty well with the most common licenses, but why limit ourselves. Lets just do every licese that SPDX has listed? It should work just as well in theory. All we need to do is remove the filter in download.py and rebuild the database and try again.

Trying out on searchcode-server again

Project License
0.929696395964 Fair Source License v0.9
0.813818153434 OCLC Research Public License 2.0
0.804549095187 OSET Public License version 2.1

0.977617793941 BSD Zero Clause License /Users/boyter/Documents/Projects/searchcode-server/include/license/database.json
0.939606278132 Attribution Assurance License /Users/boyter/Documents/Projects/searchcode-server/include/license/database.json
0.908192569643 Open CASCADE Technology Public License /Users/boyter/Documents/Projects/searchcode-server/include/license/database.json
0.902275136399 Adaptive Public License 1.0 /Users/boyter/Documents/Projects/searchcode-server/include/license/database.json
0.93217139424 JSON License /Users/boyter/Documents/Projects/searchcode-server/src/main/resources/public/js/cache.js
0.925341443302 MIT License /Users/boyter/Documents/Projects/searchcode-server/src/main/resources/public/js/cache.js
0.914039614281 feh License /Users/boyter/Documents/Projects/searchcode-server/src/main/resources/public/js/cache.js

Ok so it still picked up fair source as the main project lices which is a good result. However our very cool result of MIT being found in cache.js has gone away. Apparently the JSON license looks like the MIT license to the vector space. What to do. Indeed a diff between them shows that they are almost the same. The differences being right at the start,

MIT  License Copyright (c)               
JSON License Copyright (c) 2002 JSON.org 

and buried in the middle of the JSON license

The Software shall be used for Good, not Evil.

Hah! I remember reading about that a while ago. Something about Google not being able to use it because apparently their motto "Don't be evil" is more a guideline then a rule. https://www.cnet.com/news/dont-be-evil-google-spurns-no-evil-software/

So what we would normally do about now is add keyword weighting to the terms so that in this case MIT makes it rank higher for MIT and JSON for JSON. In fact I started doing just that with keyword it then realised with 328 licenses this is going to be a painful process. Perhaps there is a better way.

Thinking about it what we really want is to find keywords or a collection of multiple keywords that we know to be unique for each license. Then all we need to is check the text for the presense of those keywords. The catch being we need to ensure that they are unique for each license. To do so what I think will work is break the license up into collections of works of length 1-10 and check for unique-ness against the other liceses. The techical term for this is ngrams. 

An example would be,

Lorem ipsum dolor sit amet consetetur sadipscing elitr

bi-grams

[('lorem', 'ipsum'), ('ipsum', 'dolor'), ('dolor', 'sit'), ('sit', 'amet'), ('amet', 'consetetur'), ('consetetur', 'sadipscing'), ('sadipscing', 'elitr')]

tri-grams

[('lorem', 'ipsum', 'dolor'), ('ipsum', 'dolor', 'sit'), ('dolor', 'sit', 'amet'), ('sit', 'amet', 'consetetur'), ('amet', 'consetetur', 'sadipscing'), ('consetetur', 'sadipscing', 'elitr')]

Thankfully this is pretty easy to do in Python so I borrowed an existing bit of code to do it for me, http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/

input_list = ['all', 'this', 'happened', 'more', 'or', 'less']

def find_ngrams(input_list, n):
  return zip(*[input_list[i:] for i in range(n)])

For the record I am totally aware that NTLK can also do this but since I don't currently have that installed lets go pure Python. Its a little slower but considering this should rarely run calculation I am not too worried about performance yet.

We can then generate ngrams for each license, then check for its uniqueness in every other one. If no matches found then bazinga we have a gram that uniquely matches the license.

Some simple but stupid code was written parse2.py which does exactly this. Turns out that language is a lot more distinctive for licenses then I first thought,

0BSD 188
AAL 1985
Abstyles 437
Adobe-2006 1234
Adobe-Glyph 1620
ADSL 608
AFL-1.1 555
AFL-1.2 251
AFL-2.0 69
AFL-2.1 67
AFL-3.0 452
Afmparse 959
AGPL-1.0 2212

For the BSD Zero Clause License there are apparently 188 unique ngrams between a length of 2-10 words in it. For the Affero General Public License v1.0 there are a whopping 2212! In fact this seemed like overkill. So I changed the ngrams to start at 5 to 10. This dropped the numbers found by about 25% which seems about right. You would expect the most unique combinations of words to exist at the upper range. One problem I noticed with this is that a lot of the ngrams are based on names that exist within the sample licenses that SPDX has. For example BSD Zero Clause has the name "Rob Landley" which produces a lot of ngrams with this in it as is indeed unique.

However it was taking a long time to process. Not suprising really considering its multiple loop in loops. I tried just searching for ngrams of 4-5 words long, and assuming we didn't find any 0 matches than happy days we can try implementing. Doh! Alas turns out some are not unique enough. The culprits,

Artistic-1.0
BSD-3-Clause
MIT-CMU
MPL-1.1
MPL-2.0-no-copyleft-exception
MPL-2.0

I modified the code to just loop those ones with a more exaustive search to find what worked. Even with ngrams of length 2-10 still there was not enough uniqueness. So I went all out, ngrams from length 2-35.

Artistic-1.0 120
BSD-3-Clause 21

This resolved the issue for Artistic-1.0 and BSD-3-Clause but we still have nothing for the following,

MIT-CMU
MPL-1.1
MPL-2.0-no-copyleft-exception
MPL-2.0

Something is wrong here clearly. Lets look at the text of 

Turns out some of these are particullary troubling. Mozilla Public License 1.1 for example seems to be embedded to an extent in the Netscape Public License v1.1 which of course means nothing about it is unique compared to the other. The others seem to be similar. I tried searching just between both those licences with ngrams of length 2-100 to see if anything would turn up, which of course took a crazy amount of time to process in Python, however the result was that there was nothing that could be used keyword wise to seperate these licenses.

How about a hybrid approach?

If we cannot definitively say using keywords what license the file is, then we can fall back to the vector space to rank based on those without keywords. To do so I flushed the results of the last run into the database file using 7-8 ngrams for everything except Artistic-1.0 and BSD-3-Clause which checked for ngrams with a range of 2-35. The results of this produced the following licenses with their number of unique ngrams,

0BSD 25
AAL 246
Abstyles 54
Adobe-2006 163
Adobe-Glyph 210
ADSL 78
AFL-1.1 77
AFL-1.2 34
AFL-2.0 10
AFL-2.1 9
AFL-3.0 58
Afmparse 125
AGPL-1.0 298
AGPL-3.0 797
Aladdin 1386
AMDPLPA 797
AML 311
AMPAS 163
ANTLR-PD 160
Apache-1.0 100
Apache-1.1 92
Apache-2.0 64
APAFML 92
APL-1.0 6274
APSL-1.0 444
APSL-1.1 323
APSL-1.2 77
APSL-2.0 477
Artistic-1.0-cl8 7
Artistic-1.0-Perl 13
Artistic-1.0 120
Artistic-2.0 1285
Bahyph 223
Barr 96
Beerware 41
BitTorrent-1.0 66
BitTorrent-1.1 814
Borceux 59
BSD-2-Clause-FreeBSD 25
BSD-2-Clause-NetBSD 37
BSD-2-Clause 2
BSD-3-Clause-Attribution 24
BSD-3-Clause-Clear 51
BSD-3-Clause-LBNL 157
BSD-3-Clause-No-Nuclear-License-2014 27
BSD-3-Clause-No-Nuclear-License 14
BSD-3-Clause-No-Nuclear-Warranty 21
BSD-3-Clause 21
BSD-4-Clause-UC 14
BSD-4-Clause 23
BSD-Protection 733
BSD-Source-Code 29
BSL-1.0 127
bzip2-1.0.5 85
bzip2-1.0.6 51
Caldera 210
CATOSL-1.1 1939
CC-BY-1.0 3
CC-BY-2.0 3
CC-BY-2.5 3
CC-BY-3.0 3
CC-BY-4.0 3
CC-BY-NC-1.0 3
CC-BY-NC-2.0 3
CC-BY-NC-2.5 3
CC-BY-NC-3.0 3
CC-BY-NC-4.0 3
CC-BY-NC-ND-1.0 3
CC-BY-NC-ND-2.0 3
CC-BY-NC-ND-2.5 3
CC-BY-NC-ND-3.0 3
CC-BY-NC-ND-4.0 3
CC-BY-NC-SA-1.0 3
CC-BY-NC-SA-2.0 3
CC-BY-NC-SA-2.5 3
CC-BY-NC-SA-3.0 3
CC-BY-NC-SA-4.0 3
CC-BY-ND-1.0 3
CC-BY-ND-2.0 3
CC-BY-ND-2.5 3
CC-BY-ND-3.0 3
CC-BY-ND-4.0 3
CC-BY-SA-1.0 3
CC-BY-SA-2.0 3
CC-BY-SA-2.5 3
CC-BY-SA-3.0 3
CC-BY-SA-4.0 3
CC0-1.0 41
CDDL-1.0 34
CDDL-1.1 93
CECILL-1.0 1563
CECILL-1.1 3351
CECILL-2.0 76
CECILL-2.1 337
CECILL-B 300
CECILL-C 438
ClArtistic 205
CNRI-Jython 318
CNRI-Python-GPL-Compatible 236
CNRI-Python 3
Condor-1.1 627
CPAL-1.0 1037
CPL-1.0 21
CPOL-1.02 1680
Crossword 42
CrystalStacker 159
CUA-OPL-1.0 82
Cube 36
curl 16
D-FSL-1.0 2023
diffmark 11
DOC 617
Dotseqn 37
DSDP 268
dvipdfm 27
ECL-1.0 211
ECL-2.0 163
EFL-1.0 41
EFL-2.0 45
eGenix 424
Entessa 87
EPL-1.0 40
ErlPL-1.1 210
EUDatagrid 278
EUPL-1.0 319
EUPL-1.1 354
Eurosym 115
Fair 39
Frameworx-1.0 1362
FreeImage 120
FSFAP 28
FSFUL 16
FSFULLR 22
FTL 878
GFDL-1.1 380
GFDL-1.2 24
GFDL-1.3 323
Giftware 159
GL2PS 67
Glide 1263
Glulxe 75
gnuplot 163
GPL-1.0 512
GPL-2.0 102
GPL-3.0 827
gSOAP-1.3b 846
HaskellReport 92
HPND 59
IBM-pibs 134
ICU 55
IJG 629
ImageMagick 508
iMatix 520
Imlib2 93
Info-ZIP 406
Intel-ACPI 881
Intel 119
Interbase-1.0 430
IPA 1374
IPL-1.0 135
ISC 32
JasPer-2.0 182
JSON 19
LAL-1.2 766
LAL-1.3 942
Latex2e 57
Leptonica 78
LGPL-2.0 561
LGPL-2.1 813
LGPL-3.0 962
LGPLLR 949
Libpng 573
libtiff 124
LiLiQ-P-1.1 59
LiLiQ-R-1.1 148
LiLiQ-Rplus-1.1 95
LPL-1.0 104
LPL-1.02 123
LPPL-1.0 1154
LPPL-1.1 42
LPPL-1.2 50
LPPL-1.3a 100
LPPL-1.3c 203
MakeIndex 289
MirOS 347
MIT-advertising 36
>>>> MIT-CMU 0
MIT-enna 33
MIT-feh 13
MIT 4
MITNFA 80
Motosoto 367
mpich2 164
MPL-1.0 9
MPL-1.1 0
MPL-2.0-no-copyleft-exception 0
MPL-2.0 0
MS-PL 37
MS-RL 103
MTLL 253
Multics 264
Mup 42
NASA-1.3 1937
Naumen 71
NBPL-1.0 19
NCSA 63
Net-SNMP 412
NetCDF 221
Newsletr 17
NGPL 530
NLOD-1.0 1326
NLPL 33
Nokia 935
NOSL 349
Noweb 190
NPL-1.0 135
NPL-1.1 150
NPOSL-3.0 330
NRL 279
NTP 29
Nunit 34
OCCT-PL 1615
OCLC-2.0 1473
ODbL-1.0 3406
OFL-1.0 228
OFL-1.1 248
OGTSL 271
OLDAP-1.1 8
OLDAP-1.2 16
OLDAP-1.3 22
OLDAP-1.4 35
OLDAP-2.0.1 8
OLDAP-2.0 12
OLDAP-2.1 17
OLDAP-2.2.1 6
OLDAP-2.2.2 6
OLDAP-2.2 17
OLDAP-2.3 6
OLDAP-2.4 15
OLDAP-2.5 18
OLDAP-2.6 14
OLDAP-2.7 9
OLDAP-2.8 23
OML 181
OpenSSL 391
OPL-1.0 461
OSET-PL-2.1 1155
OSL-1.0 143
OSL-1.1 57
OSL-2.0 14
OSL-2.1 9
OSL-3.0 21
PDDL-1.0 2032
PHP-3.0 23
PHP-3.01 24
Plexus 74
PostgreSQL 125
psfrag 52
psutils 237
Python-2.0 394
Qhull 202
QPL-1.0 662
Rdisc 132
RHeCos-1.1 581
RPL-1.1 1065
RPL-1.5 982
RPSL-1.0 2345
RSA-MD 123
RSCPL 1029
Ruby 207
SAX-PD 250
Saxpath 113
SCEA 908
Sendmail 383
SGI-B-1.0 355
SGI-B-1.1 519
SGI-B-2.0 61
SimPL-2.0 436
SISSL-1.2 366
SISSL 269
Sleepycat 123
SMLNJ 61
SMPPL 483
SNIA 325
Spencer-86 57
Spencer-94 88
Spencer-99 113
SPL-1.0 199
SugarCRM-1.1.3 451
SWL 28
TCL 32
TCP-wrappers 68
TMate 124
TORQUE-1.1 415
TOSL 71
Unicode-DFS-2015 78
Unicode-DFS-2016 48
Unicode-TOU 923
Unlicense 123
UPL-1.0 208
Vim 752
VOSTROM 392
VSL-1.0 116
W3C-19980720 220
W3C-20150513 123
W3C 160
Watcom-1.0 823
Wsuipa 82
WTFPL 59
X11 33
Xerox 124
XFree86-1.1 100
xinetd 294
Xnet 33
xpp 138
XSkat 76
YPL-1.0 17
YPL-1.1 16
Zed 34
Zend-2.0 108
Zimbra-1.3 162
Zimbra-1.4 351
zlib-acknowledgement 39
Zlib 2
ZPL-1.1 173
ZPL-2.0 49
ZPL-2.1 81
Fair-Source-0.9 267

Most seem to have over 100 or so unique ngrams which means they will probably work pretty well, and as expected the only exceptions are the MPL licenses which has nothing unique about it compared to every other license. 

I ended up cleaning up the code at this point and improving on the loops so thankfully it took only a few minutes to run. Some of the orginal runs were taking tens of minutes due to some dodgy loops. The resulting file was on the heavy side though at 20 megabytes and crashed most editors when I tried to read it. To resolve this I truncated down to at most 50 unique ngrams per license to see how this worked in the real world. This file weighed in a much more realistic 3 megabytes.

I then created attempt2.py which used the new database using keywords to guess which license the applications had applicable. Firstly I tried it against searchcode-server itself. With the result that the LICENCE file found was indeed the Fair Source License. I then tried it against GCC. Its interesting to note that this resulted in its COPYING file being marked as containing both the GPL-2.0 and LGPL-2.1. At first I thought this might have been a bug in my logic but it seems it was actually correct. The offending ngram used to match was

"street, fifth floor, boston, ma 02110-1301 usa"

Which is supposed to be unique to LGPL-2.1 but included in this case. Since we actually have 813 (but truncated to 50) ngrams for each I figured we might as well when checking see if MOST (70%) of the keywords are there, and if so mark it as a match otherwise ignore. This resutled in the following for GCC

COPYING GPL-2.0
COPYING.LIB GPL-2.0
COPYING.RUNTIME GPL-2.0
COPYING3 GPL-2.0
COPYING3.LIB GPL-2.0

Which is correct. In fact futher tests on various other repositories worked until I hit the react-armory which is under the BSD Clear License. Turns out most of the ngrams generated for it actually involve the project name meaning they are useless. Annoyingly the ngram of length 3 "BSD Clear License" was missed becuase the parser was set to look from 7-8 ngrams. Urgh. 

Thinking about this for a bit, it totally makes sense to look for ngrams from 2-8 even when we are truncating to 50. The smaller ones are going to be high value ones usually and there shouldnt be too many of them. At the very least we should include trigrams (3 length ngrams) since it solves this issue and should work to be unqiue for a lot of licenses. Modifying the parse2.py script to take in a range of numbers defined was easy enough then just let it run and produce the new database to work with and try everything again.

Due to how painful this was getting to test manually I started to build a small test harness which I could eyeball to see if I was getting closer to a result I wanted without regressions.


Of course this isnt foolproof. We can always fall back to the vector space if we cannot find anything.



























